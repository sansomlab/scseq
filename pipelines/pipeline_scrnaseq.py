##############################################################################
#
#   Kennedy Institute of Rheumatology
#
#   $Id$
#
#   Copyright (C) 2015 Stephen Sansom
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################

"""===========================
Pipeline template
===========================

:Author: Stephen Sansom
:Release: $Id$
:Date: |today|
:Tags: Python


Overview
========

This pipeline performs the follow tasks:

(1) Mapping of reads using hisat (paired end fastq files are expected)

(2) Quantitation of gene expression 
      - Ensembl protein coding + ERCC spikes
      - Cufflinks (cuffquant + cuffnorm) is run for copy number estimation 
      - HTseq is run for counts

(3) Calculation of post-mapping QC statistics


Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_singlecell.py config


Input files
-----------

The pipeline expects sequence data from each cell in the form
of paired-end fastq files to be present in a "data.dir" 

The location of a table of ERCC spike in copy numbers should be
provided in the pipeline.ini file. 

The expected structure (tab-delimited) is:

gene_id    |genebank_id|5prime_assay  |3prime_assay  |sequence|length|copies_per_cell
ERCC-00130 |EF011072   |Ac03459943_a1 |Ac03460039_a1 |CGAT... |1059  |20324.7225


Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements (TBC):

* cufflinks
* picard
* hisat
* htseq
* R
* etc!

Pipeline output
===============

.. TBC

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys, os, glob
import sqlite3
import pandas as pd
import numpy as np

import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import CGAT.Database as DB

import PipelineScRnaseq as PipelineScRnaseq

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.


# Establish the location of module scripts for P.submit() functions
if PARAMS["code_dir"]=="":
    code_dir = os.path.dirname(os.path.realpath(__file__))
else:
    code_dir = PARAMS["code_dir"]


# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database_name"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh


# ---------------------------------------------------
# Specific pipeline tasks


###############################################################################
########################## (1) Read Mapping ###################################
###############################################################################


@follows(mkdir("annotations.dir"))
@files(os.path.join(PARAMS["annotations_dir"],"ensembl.dir","geneset_all.gtf.gz"),
       "annotations.dir/known.splice.sites.hisat.txt")
def knownHisatSpliceSites(infile, outfile):
    '''Prepare known splice junctions for hisat.
       Note that the ERCC spike-ins are not spliced'''
    
   
    statement='''extract_splice_sites.py <(zcat %(infile)s) 
                 > %(outfile)s''' % locals()

    P.run()


if PARAMS["paired"]:
    fastq_pattern = "*.fastq.1.gz"
else:
    fastq_pattern = "*.fastq.gz"
        

@follows(mkdir("hisat.dir/first.pass.dir"))    
@transform(glob.glob("data.dir/" + fastq_pattern),
          regex(r".*/(.*).fastq.*.gz"),
          add_inputs(knownHisatSpliceSites),
          r"hisat.dir/first.pass.dir/\1.novel.splice.sites.txt.gz")
def hisatFirstPass(infiles, outfile):
    '''Run a first hisat pass to identify novel splice sites'''

    reads_one, splice_sites = infiles


    
    index = PARAMS["hisat_index"]
    threads = PARAMS["hisat_threads"]
    log = outfile + ".log"
    out_name = outfile[:-len(".gz")]

    #queue options
    to_cluster= True #this is the default
    job_threads = threads
    job_options = "-l mem_free=4G"

    if PARAMS["paired"]:
        reads_two = reads_one.replace(".1.",".2.")
        fastq_input = "-1 " + reads_one + " -2 " + reads_two
    else:
        fastq_input = "-U " + reads_one
    
    statement = '''hisat
                      -x %(index)s
                      %(fastq_input)s
                      --threads %(threads)s
                      --known-splicesite-infile %(splice_sites)s
                      --novel-splicesite-outfile %(out_name)s
                      -k 20
                      -S /dev/null
                   &> %(log)s;
                   checkpoint;
                   gzip %(out_name)s;
                 ''' % locals()          

    P.run()


@merge(hisatFirstPass, "annotations.dir/novel.splice.sites.hisat.txt")
def novelHisatSpliceSites(infiles, outfile):
    '''Collect the novel splice sites into a single file'''

    junction_files = " ".join(infiles)
    
    statement = '''zcat %(junction_files)s
                   | sort -k1,1 | uniq
                   > %(outfile)s
                ''' % locals()

    P.run()

    
   
@transform(glob.glob("data.dir/" + fastq_pattern),
          regex(r".*/(.*).fastq.*.gz"),
          add_inputs(knownHisatSpliceSites, novelHisatSpliceSites),
          r"hisat.dir/\1.bam")
def hisatAlignments(infiles, outfile):
    '''Align reads using hisat with known and novel junctions'''

    reads_one, known_splice_sites, novel_splice_sites = infiles
   
    out_sam = P.getTempFilename()
    index = PARAMS["hisat_index"]
    threads = PARAMS["hisat_threads"]
    log = outfile + ".log"
    outname = outfile[:-len(".bam")]

    to_cluster= True
    job_threads = threads
    job_options = "-l mem_free=4G"

    if PARAMS["paired"]:
        reads_two = reads_one.replace(".1.",".2.")
        fastq_input = "-1 " + reads_one + " -2 " + reads_two
    else:
        fastq_input = "-U " + reads_one

    
    statement = '''hisat
                      -x %(index)s
                      %(fastq_input)s
                      --threads %(threads)s
                      --known-splicesite-infile %(known_splice_sites)s
                      --novel-splicesite-infile %(novel_splice_sites)s
                      -k 20
                      -S %(out_sam)s
                   &> %(log)s;
                   checkpoint;
                   samtools view -bS %(out_sam)s 
                   | samtools sort - %(outname)s >>%(log)s;
                   checkpoint;
                   samtools index %(outfile)s;
                   checkpoint;
                   rm %(out_sam)s;
                 ''' % locals()          

    P.run()


@follows(hisatAlignments)
def mapping():
    '''mapping target'''
    pass

    

###############################################################################
############# (2) Quantification of gene expression ###########################
###############################################################################

######################## Geneset Definition ###################################        


@follows(mkdir("annotations.dir"))
@files((os.path.join(PARAMS["annotations_dir"],
                     PARAMS["annotations_ensembl_geneset"]),
        PARAMS["annotations_ercc92_geneset"]),
        "annotations.dir/ens_ercc_geneset.gtf.gz")
def prepareEnsemblERCC92GTF(infiles, outfile):
        '''Preparation of geneset for quantitation.
           ERCC92 GTF entries are appended to 
           the protein coding entries from Ensembl geneset_all'''

        ensembl, ercc = infiles

        outname = outfile[:-len(".gz")]
        
        statement = ''' zgrep 'gene_biotype "protein_coding"' %(ensembl)s > %(outname)s;
                        checkpoint;
                        zcat %(ercc)s >> %(outname)s;
                        checkpoint;
                        gzip %(outname)s;                                                                   
                    '''
        P.run()

########################## Read Counting ######################################
        
@follows(mkdir("htseq.dir"))
@transform(hisatAlignments,
           regex(r".*/(.*).bam"),
           add_inputs(prepareEnsemblERCC92GTF),
           r"htseq.dir/\1.counts")
def runHTSeq(infiles, outfile):
    '''Run htseq-count'''
    bamfile, gtf = infiles
    statement = ''' htseq-count
                        -f bam
                        -r pos                                                                         
                        -s no                                                                          
                        -t exon
                        --quiet                                                                        
                        %(bamfile)s %(gtf)s >                                                          
                        %(outfile)s; ''' % locals()
    P.run()

@merge(runHTSeq,
       "htseq.dir/htseq_counts.load")
def loadHTSeqCounts(infiles, outfile):

        P.concatenateAndLoad(infiles, outfile,
                             regex_filename=".*/(.*).counts",
                             has_titles=False,
                             cat="track",
                             header="track,gene_id,counts",
                             options = '-i "gene_id"')
        
###################### Copynumber estimation ##################################
     
@follows(mkdir("cuffquant.dir"))
@transform(hisatAlignments,
           regex(r".*/(.*).bam"),
           add_inputs(prepareEnsemblERCC92GTF),
           r"cuffquant.dir/\1.log")

def cuffQuant(infiles, outfile):
    '''Per sample quantification using cuffquant'''

    bam_file, geneset = infiles

    # because the output of cuffquant is always "abundances.cxb"
    # a unique output directory for each sample is required
    output_dir = outfile[:-len(".log")]

    job_threads = PARAMS["cufflinks_cuffquant_threads"] # 4

    to_cluster = True

    genome_multifasta = os.path.join(PARAMS["annotations_genome_dir"], 
                                     PARAMS["genome"]+".fasta")

    gtf = P.getTempFilename()
    
    statement = '''zcat %(geneset)s > %(gtf)s;
                   checkpoint; 
                   cuffquant 
                           --output-dir %(output_dir)s
                           --num-threads %(job_threads)s
                           --multi-read-correct
                           --library-type fr-unstranded
                           --frag-bias-correct %(genome_multifasta)s
                           --no-effective-length-correction
                           --max-bundle-frags 2000000
                           --max-mle-iterations 10000
                           --verbose
                           %(gtf)s %(bam_file)s >& %(outfile)s;
                    checkpoint;
                    rm %(gtf)s;
                ''' % locals()

    P.run()


@follows(mkdir("cuffnorm.dir"), cuffQuant)
@merge([prepareEnsemblERCC92GTF, cuffQuant],
       "cuffnorm.dir/cuffnorm.log")         
def cuffNorm(infiles, outfile):
    '''Calculate FPKMs using cuffNorm'''

    # parse the infiles
    geneset = infiles[0]

    cxb_files = " ".join( [ f[:-len(".log")] + "/abundances.cxb" 
                            for f in infiles[1:] ] )

    # get the output directory and cell labels
    output_dir = os.path.dirname(outfile)

    labels  = ",".join( [f.split("/")[1]
                         for f in cxb_files.split(" ")] )

    job_options = "-l mem_free=8G"
    job_threads = PARAMS["cufflinks_cuffnorm_threads"] # 16

    gtf = P.getTempFilename()
    
    statement = ''' zcat %(geneset)s > %(gtf)s;
                    checkpoint; 
                    cuffnorm
                        --output-dir %(output_dir)s
                        --num-threads=%(job_threads)s
                        --total-hits-norm
                        --library-type=fr-unstranded
                        --library-norm-method classic-fpkm
                        --labels %(labels)s
                        %(gtf)s %(cxb_files)s > %(outfile)s;
                     checkpoint;
                     rm %(gtf)s;
                ''' % locals()

    P.run()


@transform(cuffNorm,
           suffix(".log"),
           ".load")
def loadCuffNorm(infile, outfile):
    '''load the fpkm table from cuffnorm into the database'''
    
    fpkm_table = os.path.dirname(infile) + "/genes.fpkm_table"
    
    P.load(fpkm_table, outfile,
           options = '-i "gene_id"')


@follows(mkdir("annotations.dir"))
@files(PARAMS["annotations_ercc92_info"],
       "annotations.dir/ercc.load")
def loadERCC92Info(infile, outfile):
    '''load the spike-in info including copy number'''

    P.load(infile, outfile, options='-i "gene_id"')

    
@follows(mkdir("copy.number.dir"))
@transform(cuffQuant,
           regex(r".*/(.*).log"),
           add_inputs(loadCuffNorm,
                      loadERCC92Info),
           r"copy.number.dir/\1.copynumber")
def estimateCopyNumber(infiles, outfile):
    '''Estimate copy numbers based on standard
       curves constructed from the spike-ins'''

    
    P.submit(os.path.join(code_dir,"PipelineScRnaseq.py"),
                          "estimateCopyNumber", 
                          infiles=infiles,
                          outfiles=outfile,
                          params=[code_dir])

   
@merge(estimateCopyNumber, "copy.number.dir/copynumber.load")
def loadCopyNumber(infiles, outfile):
    '''load the copy number estimations to the database'''
    
    P.concatenateAndLoad(infiles, outfile, 
                         regex_filename=".*/(.*).copynumber",
                         options = '-i "gene_id"')    


@follows(loadCopyNumber, loadHTSeqCounts)
def quantitation():
    '''quantitation target'''
    pass
    
###############################################################################
################# (3) Post-mapping Quality Control ############################
###############################################################################




@follows(mkdir("qc.dir/rnaseq.metrics.dir/"))
@transform(hisatAlignments,
           regex(r".*/(.*).bam"),
           r"qc.dir/rnaseq.metrics.dir/\1.rnaseq.metrics")
def collectRnaSeqMetrics(infile, outfile):
    '''Run Picard CollectRnaSeqMetrics on the bam files'''

    picard_out = P.getTempFilename()
    picard_options = PARAMS["picard_collectrnaseqmetrics_options"]
    
    geneset_flat = PARAMS["picard_geneset_flat"]
    strand_specificity = PARAMS["picard_strand_specificity"]
    validation_stringency = PARAMS["picard_validation_stringency"]

    job_threads = PARAMS["picard_threads"]
    job_options="-l mem_free=" + PARAMS["picard_memory"]

    coverage_out = outfile[:-len(".metrics")] + ".cov.hist"
    chart_out = outfile[:-len(".metrics")] + ".cov.pdf"
    
    statement = '''CollectRnaSeqMetrics
                   I=%(infile)s
                   REF_FLAT=%(geneset_flat)s
                   O=%(picard_out)s
                   CHART=%(chart_out)s
                   STRAND_SPECIFICITY=%(strand_specificity)s
                   VALIDATION_STRINGENCY=%(validation_stringency)s
                   %(picard_options)s;
                   checkpoint;
                   grep . %(picard_out)s | grep -v "#" | head -n2
                   > %(outfile)s;
                   checkpoint;
                   grep . %(picard_out)s 
                   | grep -A 102 "## HISTOGRAM"
                   | grep -v "##"
                   > %(coverage_out)s;
                   checkpoint;
                   rm %(picard_out)s;
                ''' % locals()

    P.run()

@merge(collectRnaSeqMetrics,
       "qc.dir/qc_rnaseq_metrics.load")
def loadCollectRnaSeqMetrics(infiles, outfile):
    '''load the metrics to the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/.*/(.*).rnaseq.metrics",
                         cat="cell",
                         options = '-i "cell"')

@transform(collectRnaSeqMetrics,
           suffix(".rnaseq.metrics"),
           ".three.prime.bias")
def threePrimeBias(infile, outfile):
    '''compute a sensible three prime bias metric
       from the picard coverage histogram'''

    coverage_histogram = infile[:-len(".metrics")] + ".cov.hist"
    
    df = pd.read_csv(coverage_histogram, sep="\t")

    x="normalized_position"
    cov="All_Reads.normalized_coverage"
    
    bias = np.mean(df[cov][(df[x]>70) & (df[x]<90)]) / np.mean(df[cov][(df[x]>20) & (df[x]<90)])

    with open(outfile, "w") as out_file:
        out_file.write("three_prime_bias\n")
        out_file.write("%.2f\n" % bias)


@merge(threePrimeBias,
       "qc.dir/qc_three_prime_bias.load")
def loadThreePrimeBias(infiles, outfile):
    '''load the metrics to the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/.*/(.*).three.prime.bias",
                         cat="cell",
                         options = '-i "cell"')

 
@follows(mkdir("qc.dir/library.complexity.dir/"))
@transform(hisatAlignments,
           regex(r".*/(.*).bam"),
           r"qc.dir/library.complexity.dir/\1.library.complexity")    
def estimateLibraryComplexity(infile, outfile):
    '''Run Picard EstimateLibraryComplexity on the bam files'''
   
    if PARAMS["paired"]:
        picard_out = P.getTempFilename()
        picard_options = PARAMS["picard_estimatelibrarycomplexity_options"]

        strand_specificity = PARAMS["picard_strand_specificity"]
        validation_stringency = PARAMS["picard_validation_stringency"]

        job_threads = PARAMS["picard_threads"]
        job_options="-l mem_free=" + PARAMS["picard_memory"]

        statement = '''EstimateLibraryComplexity
                       I=%(infile)s
                       O=%(picard_out)s
                       VALIDATION_STRINGENCY=%(validation_stringency)s
                       %(picard_options)s;
                       checkpoint;
                       grep . %(picard_out)s | grep -v "#" | head -n2
                       > %(outfile)s;
                       checkpoint;
                       rm %(picard_out)s;
                    ''' % locals()
        
    else:
        statement = '''echo "Not compatible with SE data"
                       > %(outfile)s'''

    P.run()

@merge(estimateLibraryComplexity,
       "qc.dir/qc_library_complexity.load")
def loadEstimateLibraryComplexity(infiles, outfile):
    '''load the complexity metrics to a single table in the db'''

    if PARAMS["paired"]:
        P.concatenateAndLoad(infiles, outfile,
                             regex_filename=".*/.*/(.*).library.complexity",
                             cat="cell",
                             options = '-i "cell"')
    else:
        statement = '''echo "Not compatible with SE data" 
                       > %(outfile)s'''
        P.run()


    

@follows(mkdir("qc.dir/alignment.summary.metrics.dir/"))
@transform(hisatAlignments,
           regex(r".*/(.*).bam"),
           r"qc.dir/alignment.summary.metrics.dir/\1.alignment.summary.metrics")    
def alignmentSummaryMetrics(infile, outfile):
    '''Run Picard AlignmentSummaryMetrics on the bam files'''

    picard_out = P.getTempFilename()
    picard_options = PARAMS["picard_alignmentsummarymetric_options"]
    
    strand_specificity = PARAMS["picard_strand_specificity"]
    validation_stringency = PARAMS["picard_validation_stringency"]

    job_threads = PARAMS["picard_threads"]
    job_options="-l mem_free=" + PARAMS["picard_memory"]

    reference_sequence= os.path.join(PARAMS["annotations_genome_dir"],
                                     PARAMS["genome"] + ".fasta")
                                    
    
    statement = '''CollectAlignmentSummaryMetrics
                   I=%(infile)s
                   O=%(picard_out)s
                   REFERENCE_SEQUENCE=%(reference_sequence)s
                   VALIDATION_STRINGENCY=%(validation_stringency)s
                   %(picard_options)s;
                   checkpoint;
                   grep . %(picard_out)s | grep -v "#"
                   > %(outfile)s;
                   checkpoint;
                   rm %(picard_out)s;
                ''' % locals()

    P.run()

@merge(alignmentSummaryMetrics,
       "qc.dir/qc_alignment_summary_metrics.load")
def loadAlignmentSummaryMetrics(infiles, outfile):
    '''load the complexity metrics to a single table in the db'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/.*/(.*).alignment.summary.metrics",
                         cat="cell",
                         options = '-i "cell"')

@follows(mkdir("qc.dir/uniq.mapped.reads.dir"))
@transform(hisatAlignments,
           regex(r".*/(.*).bam"),
           r"qc.dir/uniq.mapped.reads.dir/\1.uniq.mapped.reads")
def SpikeVsGenome(infile, outfile):
    '''Summarise the number of reads mapping uniquely to spike-ins and genome.
       Compute the ratio of reads mapping to spike-ins vs genome.
       Only uniquely mapping reads are considered'''
    statement= ''' echo -e "uniq_mapping_reads_genome\\tuniq_mapping_reads_spike\\tfraction_spike" > %(outfile)s;
                   checkpoint;
                   samtools view %(infile)s 
                   | grep NH:i:1 
                   | awk -v column=$3 '{OFS="\\t"} '/chr*/'{genome+=1} '/ERCC*/'{ercc+=1} END {frac=ercc/(ercc+genome);print ercc,genome,frac}' >> %(outfile)s
               ''' % locals()
    P.run()

@merge(SpikeVsGenome,
       "qc.dir/qc_uniquely_mapped_genome_spike.load")
def loadSpikeVsGenome(infiles, outfile):
    '''Load reads uniquely mapping to genome or spike-ins and fraction of spike-ins to a single db table'''
    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/.*/(.*).uniq.mapped.reads",
                         cat="cell",
                         options='-i "cell"')

#???
@follows(mkdir("qc.dir/"))
@files(loadCopyNumber,
       "qc.dir/number.genes.detected")
def numberGenesDetected(infile, outfile):
    '''Count no genes detected at copynumer > 0 in each cell'''

    table = infile.split("/")[-1][:-len(".load")]
    
    sqlstat = '''select * from %(table)s where gene_id like "ENS%%"''' % locals()
    
    df = DB.fetch_DataFrame(sqlstat, PARAMS["database_name"])
    df2 = df.pivot(index="gene_id", columns="track", values="copy_number")
    n_expressed = df2.apply(lambda x: np.sum([1 for y in x if y > 0]))
    
    n_expressed.to_csv(outfile, sep="\t")


@files(numberGenesDetected,
      "qc.dir/qc_no_genes.load")
def loadNumberGenesDetected(infile, outfile):
    '''load the numbers of genes expressed to the db'''

    P.load(infile, outfile,
           options='-i "cell" -H "cell,no_genes"')
    

@follows(mkdir("qc.dir/fraction.spliced.dir/"))
@transform(hisatAlignments,
           regex(r".*/(.*).bam"),
           r"qc.dir/fraction.spliced.dir/\1.fraction.spliced")        
def fractionReadsSpliced(infile, outfile):
    '''Compute fraction of reads containing a splice junction.
       * paired-endedness is ignored
       * only uniquely mapping reads are considered'''

    statement='''echo "fraction_spliced" > %(outfile)s;
                 checkpoint;
                 samtools view %(infile)s 
                 | grep NH:i:1 
                 | cut -f 6
                 | awk '{if(index($1,"N")==0){us+=1}else{s+=1}}END{print s/(us+s)}'
                 >> %(outfile)s
               ''' % locals()

    P.run()

@merge(fractionReadsSpliced,
       "qc.dir/qc_fraction_spliced.load")
def loadFractionReadsSpliced(infiles, outfile):
    '''load to fractions of spliced reads to a single db table'''

    P.concatenateAndLoad(infiles, outfile,
                         regex_filename=".*/.*/(.*).fraction.spliced",
                         cat="cell",
                         options = '-i "cell"')


@merge([loadCollectRnaSeqMetrics,
        loadThreePrimeBias,
        loadEstimateLibraryComplexity,
        loadSpikeVsGenome,
        loadFractionReadsSpliced,
        loadNumberGenesDetected,
        loadAlignmentSummaryMetrics],
       "qc.dir/qc_summary.txt")       
def qcSummary(infiles, outfile):
    '''create a summary table of relevant QC metrics'''

    # Some QC metrics are specific to paired end data
    if PARAMS["paired"]:
        exclude = []
        optional_columns = '''READ_PAIRS_EXAMINED as no_pairs,
                              PERCENT_DUPLICATION as percent_duplication,
                              ESTIMATED_LIBRARY_SIZE as library_size,
                           '''
        pcat = "PAIR"
        
    else:
        exclude = ["qc_library_complexity"]
        optional_columns = ''
        pcat = "UNPAIRED"

    tables = [P.toTable(x) for x in infiles
              if P.toTable(x) not in exclude]

    t1 = tables[0]

    join_stat = ""
    for table in tables[1:]:
        join_stat += "left join " + table + "\n"
        join_stat += "on " + t1 + ".cell=" + table + ".cell\n"

        
    stat_start = '''select distinct %(t1)s.cell, 
                                   fraction_spliced, 
                                   fraction_spike, 
                                   no_genes, 
                                   three_prime_bias as three_prime_bias,
                                   uniq_mapping_reads_genome,
                                   uniq_mapping_reads_spike,
                                   %(optional_columns)s
                                   PCT_MRNA_BASES as percent_mrna,
                                   PCT_CODING_BASES as percent_coding,
                                   PCT_PF_READS_ALIGNED as percent_reads_aligned,
                                   TOTAL_READS as total_reads,
                                   PCT_PF_READS as pct_pf_reads,
                                   PCT_PF_READS_ALIGNED as pct_pf_reads_aligned,
                                   PCT_READS_ALIGNED_IN_PAIRS as pct_reads_aligned_in_pairs
                   from %(t1)s
                ''' % locals()


    where_stat = '''where qc_alignment_summary_metrics.CATEGORY="%(pcat)s"
                 ''' % locals()
    
    statement = "\n".join([stat_start, join_stat, where_stat])

    print statement
    
    df = DB.fetch_DataFrame(statement, PARAMS["database_name"])
    df.to_csv(outfile, sep="\t", index=False)


@transform(qcSummary,
           suffix(".txt"),
           ".load")
def loadQCSummary(infile, outfile):
    '''load summary to db'''

    P.load(infile, outfile)

    
@follows(loadQCSummary)
def qc():
    '''target for executing qc'''
    pass
    
    
# ---------------------------------------------------
# Generic pipeline tasks
@follows(mapping, quantitation, qc)
def full():
    pass

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
